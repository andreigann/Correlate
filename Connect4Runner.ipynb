{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Connect4Runner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreigann/Correlate/blob/master/Connect4Runner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-QV5r6HIqvd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ed9de32-2088-40e7-90a3-e5ad57f0c04e"
      },
      "source": [
        "!pip install pygame"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pygame in /usr/local/lib/python3.6/dist-packages (1.9.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nO6fn6yKKSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zQDNopMKOSA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4bd016a-3ace-4f94-81b0-337461920bdb"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3adkHqwLVj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/Colab Notebooks/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSCZQ5eaM1Ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym_connect_four import RandomPlayer, ConnectFourEnv, Player, SavedPlayer\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQhqFMfqVWW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyRandomPlayer(Player):\n",
        "    def __init__(self, env, name='MyRandomPlayer', id=None):\n",
        "        super(MyRandomPlayer, self).__init__(env, name)\n",
        "        self.id=id\n",
        "        self.wins=0\n",
        "        self.draws=0\n",
        "        self.losses=0\n",
        "\n",
        "        self.main_score_wins = 0\n",
        "        self.main_score_draws = 0\n",
        "        self.main_score_losses = 0\n",
        "\n",
        "    def get_next_action(self, state: np.ndarray) -> int:\n",
        "      am = self.env.available_moves()\n",
        "\n",
        "      for _ in range(100):\n",
        "        action = np.random.randint(self.env.action_space.n)\n",
        "        if self.env.is_valid_action(action):\n",
        "            return action\n",
        "      raise Exception('Unable to determine a valid move! Maybe invoke at the wrong time?')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w3hcPnchhwA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNSolver:\n",
        "  \"\"\"\n",
        "  Vanilla Multi Layer Perceptron version\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, observation_space, action_space):\n",
        "    self.GAMMA = 0.95\n",
        "    self.LEARNING_RATE = 0.001\n",
        "\n",
        "    self.MEMORY_SIZE = 1000000\n",
        "    self.BATCH_SIZE = 20\n",
        "\n",
        "    self.EXPLORATION_MAX = 1.0\n",
        "    self.EXPLORATION_MIN = 0.01\n",
        "    self.EXPLORATION_DECAY = 0.995\n",
        "\n",
        "    self.exploration_rate = self.EXPLORATION_MAX\n",
        "\n",
        "    self.action_space = action_space\n",
        "    self.memory = deque(maxlen=self.MEMORY_SIZE)\n",
        "\n",
        "    self.model = Sequential()\n",
        "    self.model.add(Flatten(input_shape=observation_space))\n",
        "    self.model.add(Dense(24, activation=\"relu\"))\n",
        "    self.model.add(Dense(24, activation=\"relu\"))\n",
        "    self.model.add(Dense(self.action_space, activation=\"linear\"))\n",
        "    self.model.compile(loss=\"mse\", optimizer=Adam(lr=self.LEARNING_RATE))\n",
        "\n",
        "  def remember(self, state, action, reward, next_state, done):\n",
        "    self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "  def act(self, state, available_moves=[]):\n",
        "    if np.random.rand() < self.exploration_rate:\n",
        "      return random.randrange(self.action_space)\n",
        "    q_values = self.model.predict(state)\n",
        "    q_values = np.array([[x if idx in available_moves else -100 for idx, x in enumerate(q_values[0])]])\n",
        "    return np.argmax(q_values[0])\n",
        "\n",
        "  def experience_replay(self):\n",
        "    if len(self.memory) < self.BATCH_SIZE:\n",
        "      return\n",
        "    batch = random.sample(self.memory, self.BATCH_SIZE)\n",
        "    for state, action, reward, state_next, terminal in batch:\n",
        "      q_update = reward\n",
        "      if not terminal:\n",
        "        q_update = (reward + self.GAMMA * np.amax(self.model.predict(state_next)[0]))\n",
        "      q_values = self.model.predict(state)\n",
        "      q_values[0][action] = q_update\n",
        "      self.model.fit(state, q_values, verbose=0)\n",
        "    self.exploration_rate *= self.EXPLORATION_DECAY\n",
        "    self.exploration_rate = max(self.EXPLORATION_MIN, self.exploration_rate)\n",
        "\n",
        "  def save_model(self, file_prefix: str):\n",
        "    self.model.save(f\"{file_prefix}.h5\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xYXIw2gWJRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyNNPlayer(Player):\n",
        "  def __init__(self, env, name='MyNNPlayer', id=None):\n",
        "    super(MyNNPlayer, self).__init__(env, name)\n",
        "    self.observation_space = env.observation_space.shape\n",
        "    self.action_space = env.action_space.n\n",
        "\n",
        "    self.dqn_solver = DQNSolver(self.observation_space, self.action_space)\n",
        "\n",
        "    self.id=id\n",
        "    self.wins=0\n",
        "    self.draws=0\n",
        "    self.losses=0\n",
        "\n",
        "    self.main_score_wins = 0\n",
        "    self.main_score_draws = 0\n",
        "    self.main_score_losses = 0\n",
        "\n",
        "  def get_next_action(self, state: np.ndarray) -> int:\n",
        "    state = np.reshape(state, [1] + list(self.observation_space))\n",
        "    action = self.dqn_solver.act(state, self.env.available_moves())\n",
        "    return action\n",
        "\n",
        "  def learn(self, state, action, reward, state_next, done) -> None:\n",
        "    state = np.reshape(state, [1] + list(self.observation_space))\n",
        "    state_next = np.reshape(state_next, [1] + list(self.observation_space))\n",
        "\n",
        "    # reward = reward if not done else -reward\n",
        "    self.dqn_solver.remember(state, action, reward, state_next, done)\n",
        "\n",
        "    if not done:\n",
        "      self.dqn_solver.experience_replay()\n",
        "\n",
        "  def save_model(self):\n",
        "    self.dqn_solver.save_model(self.name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAgG1KOKWLik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyCNNPlayer(Player):\n",
        "  def __init__(self, env, name='MyCNNPlayer', id=None):\n",
        "    super(MyCNNPlayer, self).__init__(env, name)\n",
        "    self.id=id\n",
        "    self.wins=0\n",
        "    self.draws=0\n",
        "    self.losses=0\n",
        "\n",
        "    self.main_score_wins = 0\n",
        "    self.main_score_draws = 0\n",
        "    self.main_score_losses = 0\n",
        "\n",
        "  def get_next_action(self, state: np.ndarray) -> int:\n",
        "    for _ in range(100):\n",
        "      action = np.random.randint(self.env.action_space.n)\n",
        "      if self.env.is_valid_action(action):\n",
        "        return action\n",
        "      raise Exception('Unable to determine a valid move! Maybe invoke at the wrong time?')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5RObidogwQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def create_random_player(i)-> MyRandomPlayer:\n",
        "  return MyRandomPlayer(env, name=('OpponentRandomPlayer' + str(i)), id=i)\n",
        "\n",
        "def create_nn_player(i)-> MyNNPlayer:\n",
        "  return MyNNPlayer(env, name=('NNPlayer' + str(i)), id=i)\n",
        "\n",
        "def create_cnn_player(i)-> MyCNNPlayer:\n",
        "  return MyCNNPlayer(env, name=('CNNPlayer' + str(i)), id=i)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bimL61m2g2ll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_a_game(player1, player2, rounds=ROUNDS):\n",
        "  # print(f\"{player1.name} vs {player2.name}\")\n",
        "\n",
        "  result = [0] * 3\n",
        "  for episodes in range(rounds):\n",
        "    match_result = None\n",
        "\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    p2 = False\n",
        "    while not done:\n",
        "        action1 = player1.get_next_action(state)\n",
        "        state1, reward1, done1, _ = env.step(action1)\n",
        "\n",
        "        if p2:\n",
        "            player2.learn(state1, action2, env._reverse_reward(reward1), state, done1)\n",
        "        else:\n",
        "            p2 = True\n",
        "\n",
        "        if not done1:\n",
        "            action2 = player2.get_next_action(state1)\n",
        "            state2, reward2, done2, _ = env.step(action2)\n",
        "\n",
        "            player1.learn(state, action1, env._reverse_reward(reward2), state2, done2)\n",
        "\n",
        "            if done2:\n",
        "                done = True\n",
        "                player2.learn(state1, action2, reward2, state2, done2)\n",
        "                if reward2 != env.DRAW_REWARD:\n",
        "                    # player2 Won\n",
        "                    player2.wins=player2.wins+1\n",
        "                    player1.losses=player1.losses+1;\n",
        "                else:\n",
        "                    # player2 Draw\n",
        "                    player1.draws=player1.draws+1\n",
        "                    player2.draws=player2.draws+1\n",
        "\n",
        "            state = state2\n",
        "        else:\n",
        "            done = True\n",
        "            player1.learn(state, action1, reward1, state1, done1)\n",
        "            if reward1 != env.DRAW_REWARD:\n",
        "                # player1 Won\n",
        "                  player1.wins=player1.wins+1\n",
        "                  player2.losses=player2.losses+1;\n",
        "            else:\n",
        "                # player1 Draw\n",
        "                  player1.draws=player1.draws+1\n",
        "                  player2.draws=player2.draws+1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdTYjNPIQYKl",
        "colab_type": "code",
        "outputId": "5bc52ee9-c034-4df6-9c22-4a66d55cb748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "ROUNDS=int(10)\n",
        "NN_PLAYERS=int(1)\n",
        "RANDOM_PLAYERS=int(1)\n",
        "CNN_PLAYERS=int(1)\n",
        "ENV_NAME = \"ConnectFour-v0\"\n",
        "TRAIN_EPISODES = 1000\n",
        "env = gym.make(ENV_NAME)\n",
        "\n",
        "def train():\n",
        "  players = []\n",
        "  games_to_play = []\n",
        "\n",
        "  for i in range(1, NN_PLAYERS+1):\n",
        "    players.append(create_nn_player(i))\n",
        "\n",
        "  for i in range(1, RANDOM_PLAYERS+1):\n",
        "    players.append(create_random_player(i))\n",
        "\n",
        "  for i in range(1, CNN_PLAYERS+1):\n",
        "    players.append(create_cnn_player(i))\n",
        "\n",
        "  random.shuffle(players)\n",
        "\n",
        "  for p1 in players:\n",
        "    for p2 in players:\n",
        "      if p1 == p2:\n",
        "        continue\n",
        "      games_to_play.append((p1, p2))\n",
        "\n",
        "  for game in games_to_play:\n",
        "    player1 = game[0]\n",
        "    player2 = game[1]\n",
        "    print(player1.name, player2.name)\n",
        "\n",
        "    play_a_game(player1, player2, ROUNDS)\n",
        "\n",
        "    print(player1.wins, player2.wins)\n",
        "\n",
        "    if player1.wins > player2.wins:\n",
        "      player1.main_score_wins = player1.main_score_wins + 1\n",
        "      player2.main_score_losses = player2.main_score_losses + 1\n",
        "\n",
        "    if (player1.wins < player2.wins):\n",
        "      player2.main_score_wins = player2.main_score_wins + 1\n",
        "      player1.main_score_losses = player1.main_score_losses + 1\n",
        "\n",
        "    if (player1.wins == player2.wins):\n",
        "      player1.main_score_draws = player1.main_score_draws + 1\n",
        "      player2.main_score_draws = player2.main_score_draws + 1\n",
        "\n",
        "    player1.wins = 0\n",
        "    player2.wins = 0\n",
        "    player1.draws = 0\n",
        "    player2.draws = 0\n",
        "    player1.losses = 0\n",
        "    player2.losses = 0\n",
        "  \n",
        "  for p in players:\n",
        "    print(p.name, \": \", p.main_score_wins, p.main_score_draws, p.main_score_losses)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        train()"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OpponentRandomPlayer1 NNPlayer1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-157-0959e8314466>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-157-0959e8314466>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mplay_a_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mROUNDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-154-dbee77ac78cb>\u001b[0m in \u001b[0;36mplay_a_game\u001b[0;34m(player1, player2, rounds)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0maction2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplayer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mstate2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mplayer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reverse_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/My Drive/Colab Notebooks/gym_connect_four/envs/connect_four_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopponent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/My Drive/Colab Notebooks/gym_connect_four/envs/connect_four_env.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_valid_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             raise Exception(\n\u001b[0;32m--> 150\u001b[0;31m                 \u001b[0;34m'Unable to determine a valid move! Maybe invoke at the wrong time?'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Unable to determine a valid move! Maybe invoke at the wrong time?"
          ]
        }
      ]
    }
  ]
}